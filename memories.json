{
  "schema_version": "1.0.0",
  "project": "",
  "created": "",
  "last_updated": "",
  "memories": [],
  "_schema": {
    "description": "Each memory object in the memories array follows this structure",
    "memory": {
      "id": "string — unique identifier (e.g., 'mem_001')",
      "type": "string — 'observation' | 'pattern' | 'decision' | 'false_belief' | 'drift_event'",
      "content": "string — the memory itself, written as a factual statement",
      "context": "string — what session/task produced this memory",
      "source": "string — 'onboarding' | 'cross_check' | 'review' | 'human' | 'self_reported'",
      "confidence": {
        "score": "number 0-1 — how reliable is this memory",
        "reasoning": "string — why this confidence level"
      },
      "reinforcement_count": "number — how many independent sessions have confirmed this",
      "last_reinforced": "string — ISO date of most recent reinforcement",
      "created": "string — ISO date",
      "tags": ["string — domain tags for contextual loading"],
      "related_memories": ["string — IDs of related memories"],
      "status": "string — 'active' | 'deprecated' | 'challenged'"
    },
    "types_explained": {
      "observation": "A specific fact about this codebase or domain. Example: 'The auth module is tightly coupled to session middleware — changes cascade.'",
      "pattern": "A recurring behavioral tendency of the AI. Example: 'AI tends to declare completion before verifying all success criteria are met.'",
      "decision": "An architectural or design decision and its rationale. Example: 'Chose eventual consistency for the event store because write throughput matters more than read freshness.'",
      "false_belief": "Something the AI stated confidently that turned out to be wrong. Example: 'AI claimed the caching layer was write-through when it was actually write-behind. Propagated for 3 sessions before caught.'",
      "drift_event": "A documented instance where execution diverged from the plan. Example: 'Plan specified REST endpoints but implementation silently switched to GraphQL without surfacing the decision.'"
    },
    "scoring_rules": {
      "confidence_updates": "Reinforcement increases confidence toward 1.0. Contradiction decreases it toward 0. Formula: new_score = old_score + (direction * 0.1 * (1 - old_score)) for increases, old_score * 0.8 for decreases.",
      "reinforcement": "A memory is reinforced when an independent session confirms the same observation. Only cross_check, review, or human sources can reinforce. Self-reported cannot reinforce itself.",
      "deprecation": "A memory is deprecated when its confidence drops below 0.2 or when a human explicitly marks it obsolete. Deprecated memories remain in the store but are not loaded into active context.",
      "decay": "Memories that haven't been reinforced in 10+ sessions have their confidence reduced by 0.05 per session. This prevents stale observations from persisting indefinitely.",
      "false_belief_weight": "false_belief type memories carry 2x weight in context loading. The AI's past confident mistakes are the most important things to remember."
    },
    "context_loading": {
      "priority_order": [
        "1. false_beliefs (always loaded, 2x weight)",
        "2. active patterns sorted by confidence * reinforcement_count",
        "3. drift_events from last 5 sessions",
        "4. decisions (loaded when tags match current session intent)",
        "5. observations (loaded when tags match current session intent)"
      ],
      "budget": "Load memories until context budget is consumed. Default: top 30 memories by priority score. Adjust per session if needed."
    }
  }
}
